{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da3c678",
   "metadata": {},
   "source": [
    "# Empathy and Distress Inference with EmotionStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917ac4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelWithHeads\n",
    "import torch\n",
    "from utils import utils\n",
    "from transformers import RobertaTokenizer\n",
    "import transformers.adapters.composition as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b200e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load data \"\"\"\n",
    "\n",
    "train_data, val_data, test_data = utils.load_wassa_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84657b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/alahnala/miniconda3/envs/st/lib/python3.9/site-packages/transformers/adapters/models/roberta.py:250: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/app/home/alahnala/miniconda3/envs/st/lib/python3.9/site-packages/transformers/adapters/models/roberta.py:228: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "model = AutoModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cb4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load adapters \"\"\"\n",
    "empathy_adapter_path = \"./trained_adapters/EMP_emotion_stack\"\n",
    "distress_adapter_path = \"./trained_adapters/DIS_emotion_stack\"\n",
    "\n",
    "empathy_adapter = model.load_adapter(empathy_adapter_path, load_as=empathy_adapter_path.split('/')[-1])\n",
    "distress_adapter = model.load_adapter(distress_adapter_path, load_as=distress_adapter_path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd72d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" activate \"\"\"\n",
    "model.set_active_adapters(ac.Parallel(empathy_adapter, distress_adapter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99207840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_empathy_distress(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    input_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokens))\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    emp = outputs[0].logits[0][0].tolist()\n",
    "    dis = outputs[1].logits[0][0].tolist()\n",
    "    return emp, dis\n",
    "\n",
    "\n",
    "def predict_and_print(sentence):\n",
    "    empathy_score, distress_score = pred_empathy_distress(sentence)\n",
    "    \n",
    "    print(\"Essay: \")\n",
    "    print(sentence)\n",
    "    print(f\"Empathy: {empathy_score}\\tDistress: {distress_score}\")\n",
    "    \n",
    "    return empathy_score, distress_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc257672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essay: \n",
      "I think that this is pretty sad.  I definitely think that there should be a lot more empathy for these people.  Say what you will, but to me it's just another example of the haves keeping the have nots as far down as possible.  I wonder how this article makes you feel and I am really curious to hear your opinions on the matter.\n",
      "Empathy: 4.637167453765869\tDistress: 3.3121418952941895\n",
      "\n",
      "Actuals\n",
      "Empathy: 6.0\tDistress: 1.0\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "\n",
    "essay = val_data['essay'].values[idx]\n",
    "empathy_score, distress_score = predict_and_print(essay)\n",
    "\n",
    "print(f\"\\nActuals\\nEmpathy: {val_data['empathy'].values[idx]}\\tDistress: {val_data['distress'].values[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
